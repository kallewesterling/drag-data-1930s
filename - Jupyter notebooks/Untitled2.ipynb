{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n",
      "Generating networks: 6/6 \u001b[32m██████████\u001b[0m\n",
      "Adding edges: 12/12 \u001b[32m██████████\u001b[0m\n",
      "Adding metadata: 12/12 \u001b[32m██████████\u001b[0m\n",
      "Generating metadata for connected nodes in each network: 12/12 \u001b[32m██████████\u001b[0m\n",
      "Generating modularities for network: 0/12 \u001b[32m          \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       --> Louvain for `grouped-by-3-days`\n",
      "       --> CNM for `grouped-by-3-days`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating modularities for network: 1/12 \u001b[32m▊         \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       --> Louvain for `grouped-by-3-days-no-unnamed-performers`\n",
      "       --> CNM for `grouped-by-3-days-no-unnamed-performers`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating modularities for network: 2/12 \u001b[32m█▋        \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       --> Louvain for `grouped-by-14-days`\n",
      "       --> CNM for `grouped-by-14-days`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating modularities for network: 3/12 \u001b[32m██▌       \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       --> Louvain for `grouped-by-14-days-no-unnamed-performers`\n",
      "       --> CNM for `grouped-by-14-days-no-unnamed-performers`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating modularities for network: 4/12 \u001b[32m███▎      \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       --> Louvain for `grouped-by-31-days`\n",
      "       --> CNM for `grouped-by-31-days`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating modularities for network: 5/12 \u001b[32m████▏     \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       --> Louvain for `grouped-by-31-days-no-unnamed-performers`\n",
      "       --> CNM for `grouped-by-31-days-no-unnamed-performers`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating modularities for network: 6/12 \u001b[32m█████     \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       --> Louvain for `grouped-by-93-days`\n",
      "       --> CNM for `grouped-by-93-days`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating modularities for network: 7/12 \u001b[32m█████▊    \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       --> Louvain for `grouped-by-93-days-no-unnamed-performers`\n",
      "       --> CNM for `grouped-by-93-days-no-unnamed-performers`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating modularities for network: 8/12 \u001b[32m██████▋   \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       --> Louvain for `grouped-by-186-days`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating modularities for network: 8/12 \u001b[32m██████▋   \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       --> CNM for `grouped-by-186-days`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating modularities for network: 9/12 \u001b[32m███████▌  \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       --> Louvain for `grouped-by-186-days-no-unnamed-performers`\n",
      "       --> CNM for `grouped-by-186-days-no-unnamed-performers`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating modularities for network: 10/12 \u001b[32m████████▎ \u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       --> Louvain for `grouped-by-365-days`\n",
      "       --> CNM for `grouped-by-365-days`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating modularities for network: 11/12 \u001b[32m█████████▏\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       --> Louvain for `grouped-by-365-days-no-unnamed-performers`\n",
      "       --> CNM for `grouped-by-365-days-no-unnamed-performers`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating modularities for network: 12/12 \u001b[32m██████████\u001b[0m\n",
      "Setting modularities on network metadata: 12/12 \u001b[32m██████████\u001b[0m\n",
      "Setting degrees on network metadata: 12/12 \u001b[32m██████████\u001b[0m\n",
      "Correcting last-minute data for networks: 12/12 \u001b[32m██████████\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import community as community_louvain\n",
    "import copy\n",
    "from colorama import Fore, Style\n",
    "import networkx as nx\n",
    "from collections import Counter\n",
    "import unicodedata\n",
    "import re\n",
    "import pandas as pd\n",
    "import json\n",
    "import jellyfish\n",
    "from pathlib import Path\n",
    "import datetime\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from IPython.display import display, HTML, Markdown, clear_output\n",
    "except ModuleNotFoundError:\n",
    "    print(\"No IPython found.\")\n",
    "\n",
    "settings = {\"DAYSPANS\": [3, 14, 31, 93, 186, 365]}\n",
    "urls = [\n",
    "    {\n",
    "        \"prefix\": \"v1\",\n",
    "        \"url\": \"https://docs.google.com/spreadsheets/d/e/2PACX-1vT0E0Y7txIa2pfBuusA1cd8X5OVhQ_D0qZC8D40KhTU3xB7McsPR2kuB7GH6ncmNT3nfjEYGbscOPp0/pub?gid=254069133&single=true&output=csv\",\n",
    "    },\n",
    "    {\n",
    "        \"prefix\": \"live\",\n",
    "        \"url\": \"https://docs.google.com/spreadsheets/d/e/2PACX-1vT0E0Y7txIa2pfBuusA1cd8X5OVhQ_D0qZC8D40KhTU3xB7McsPR2kuB7GH6ncmNT3nfjEYGbscOPp0/pub?gid=0&single=true&output=csv\",\n",
    "    },\n",
    "]\n",
    "\n",
    "\n",
    "def in_notebook():\n",
    "    try:\n",
    "        from IPython import get_ipython\n",
    "\n",
    "        try:\n",
    "            if \"IPKernelApp\" not in get_ipython().config:\n",
    "                return False\n",
    "        except AttributeError:\n",
    "            return False\n",
    "    except ImportError:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "\n",
    "def log(msg, color=\"green\", verbose=True):\n",
    "    now = datetime.datetime.now().strftime(\"%H:%M%:%S\")\n",
    "    if verbose and in_notebook():\n",
    "        return display(Markdown(f'<font color=\"{color}\">[{now}] {msg}</font>'))\n",
    "    elif verbose:\n",
    "        return print(f\"[{now}]:\\n{msg}\\n\\n\")\n",
    "    return None\n",
    "\n",
    "\n",
    "def slugify(value, allow_unicode=False, verbose=False):\n",
    "    init_value = str(value)\n",
    "    value = init_value\n",
    "    value = (\n",
    "        unicodedata.normalize(\"NFKD\", value).encode(\"ascii\", \"ignore\").decode(\"ascii\")\n",
    "    )\n",
    "    value = re.sub(r\"[^\\w\\s-]\", \"\", value.lower())\n",
    "    value = re.sub(r\"^(\\d+)\", r\"n\\1\", value)\n",
    "    value = re.sub(r\"[-\\s]+\", \"_\", value).strip(\"-_\")\n",
    "    if verbose:\n",
    "        clear_output(wait=True)\n",
    "        log(f\"Making slug from {init_value}: {value}\", verbose=verbose)\n",
    "    return value\n",
    "\n",
    "\n",
    "def get_raw_data(\n",
    "    verbose=True,\n",
    "    url=\"https://docs.google.com/spreadsheets/d/e/2PACX-1vT0E0Y7txIa2pfBuusA1cd8X5OVhQ_D0qZC8D40KhTU3xB7McsPR2kuB7GH6ncmNT3nfjEYGbscOPp0/pub?gid=254069133&single=true&output=csv\",\n",
    "):\n",
    "    df = pd.read_csv(url)\n",
    "\n",
    "    df.replace(\"—\", \"\", inplace=True)\n",
    "    df.replace(\"—*\", \"\", inplace=True)\n",
    "    df.replace(\"–\", \"\", inplace=True)\n",
    "    df.fillna(\"\", inplace=True)\n",
    "\n",
    "    log(f\"**{df.shape[0]} rows imported.**\", verbose=verbose)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def filter_data(df, min_date=None, max_date=None, verbose=True, skip_unsure=False):\n",
    "    def has_required_data(row):\n",
    "        \"\"\"(internal) for use with DataFrame lambda function to ensure that any given row has the required data present\"\"\"\n",
    "        has_performer = (\n",
    "            row[\"Performer\"] != \"\"\n",
    "            or row[\"Normalized performer\"] != \"\"\n",
    "            or (row[\"Performer first-name\"] != \"\" or row[\"Performer last-name\"]) != \"\"\n",
    "        )\n",
    "        has_venue = row[\"Venue\"] != \"\"\n",
    "        if has_performer and has_venue:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    def has_correct_date(row):\n",
    "        \"\"\"(internal) for use with DataFrame lambda function to ensure that any given row has a correct date present\"\"\"\n",
    "        return re.search(r\"\\d{4}\\-\\d{2}\\-\\d{2}\", row[\"Date\"]) != None\n",
    "\n",
    "    def string_date(row):\n",
    "        return row[\"Date\"].strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    df = df.copy()\n",
    "\n",
    "    df[\"has_required_data\"] = df.apply(lambda row: has_required_data(row), axis=1)\n",
    "    df.drop(df[df[\"has_required_data\"] == False].index, inplace=True)\n",
    "    log(f\"**{df.shape[0]} rows after filtering**: Required data.\", verbose=verbose)\n",
    "\n",
    "    df.drop(df[df[\"Exclude from visualization\"] == True].index, inplace=True)\n",
    "    df.drop(df[df[\"Exclude from visualization\"] == \"TRUE\"].index, inplace=True)\n",
    "    log(\n",
    "        f\"**{df.shape[0]} rows after filtering**: Exclusion from visulization.\",\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    if skip_unsure == False:\n",
    "        df.drop(df[df[\"Unsure whether drag artist\"] == True].index, inplace=True)\n",
    "        df.drop(df[df[\"Unsure whether drag artist\"] == \"TRUE\"].index, inplace=True)\n",
    "        log(\n",
    "            f\"**{df.shape[0]} rows after filtering**: Unsure whether drag artist.\",\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    df[\"has_correct_date\"] = df.apply(lambda row: has_correct_date(row), axis=1)\n",
    "    df.drop(df[df[\"has_correct_date\"] == False].index, inplace=True)\n",
    "    log(\n",
    "        f\"**{df.shape[0]} rows after filtering**: Full date in `Date` column.\",\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    if min_date or max_date:\n",
    "        df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "        df = df[(df[\"Date\"] > min_date) & (df[\"Date\"] < max_date)]\n",
    "        df[\"Date\"] = df.apply(lambda row: string_date(row), axis=1)\n",
    "        log(\n",
    "            f\"**{df.shape[0]} rows after filtering**: Min and max date set.\",\n",
    "            verbose=verbose,\n",
    "        )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def clean_data(df, drop_cols=[], verbose=True, forbidden=[\"?\", \"[\", \"]\"]):\n",
    "    def get_performer(row, null_value=\"\"):\n",
    "        \"\"\"(internal) for use with DataFrame lambda function to return the cleaned-up version of a performer's name (in an order of priority)\"\"\"\n",
    "\n",
    "        first_name = row[\"Performer first-name\"]\n",
    "        last_name = row[\"Performer last-name\"]\n",
    "\n",
    "        returnVal = None\n",
    "\n",
    "        if not returnVal and (last_name and not first_name):\n",
    "            returnVal = last_name\n",
    "\n",
    "        if not returnVal and (\n",
    "            row[\"Normalized performer\"]\n",
    "            and not \"—\" in row[\"Normalized performer\"]\n",
    "            and not \"–\" in row[\"Normalized performer\"]\n",
    "        ):\n",
    "            returnVal = row[\"Normalized performer\"]\n",
    "\n",
    "        if not returnVal and (first_name and last_name):\n",
    "            if not \"—\" in first_name and not \"—\" in last_name:\n",
    "                returnVal = f\"{first_name} {last_name}\"\n",
    "\n",
    "            elif not \"—\" in last_name and \"—\" in first_name:\n",
    "                returnVal = last_name\n",
    "\n",
    "            elif not \"—\" in first_name and \"—\" in last_name:\n",
    "                returnVal = first_name\n",
    "\n",
    "        if not returnVal and row[\"Performer\"]:\n",
    "            returnVal = row[\"Performer\"]\n",
    "\n",
    "        if not returnVal:\n",
    "            return null_value\n",
    "\n",
    "        return \"\".join([x for x in returnVal if not x in forbidden])\n",
    "\n",
    "    def get_city(row, null_value=\"\"):\n",
    "        \"\"\"(internal) for use with DataFrame lambda function to return the cleaned-up version of a city's name (in an order of priority)\"\"\"\n",
    "        for r in [\"Normalized City\", \"City\"]:\n",
    "            if row[r]:\n",
    "                return row[r]\n",
    "\n",
    "        return null_value\n",
    "\n",
    "    def get_unique_venue(row, null_value=\"\"):\n",
    "        \"\"\"(internal) for use with DataFrame lambda function to return the cleaned-up version of a venue's name (in an order of priority)\"\"\"\n",
    "        if row[\"Venue\"] and row[\"City\"]:\n",
    "            return row[\"Venue\"] + \" (\" + row[\"City\"] + \")\"\n",
    "\n",
    "        for r in [\"Venue\", \"City\"]:\n",
    "            if row[r]:\n",
    "                return row[r]\n",
    "\n",
    "        return null_value\n",
    "\n",
    "    def get_source(row, null_value=\"\"):\n",
    "        \"\"\"(internal) for use with DataFrame lambda function to return the cleaned-up version of a source (in an order of priority)\"\"\"\n",
    "        for r in [\"Source clean\", \"Source\"]:\n",
    "            if row[r]:\n",
    "                g = re.search(\n",
    "                    r\"(Jan|Feb|Mar|Apr|May|Jun|Jul|Aug|Sep|Oct|Nov|Dec)\", row[r]\n",
    "                )\n",
    "                if not g:\n",
    "                    g = re.search(r\"\\d{4}-\\d{2}-\\d{2}\", row[r])\n",
    "                    if not g:\n",
    "                        return f\"{row[r]} ({datetime.datetime.strptime(row['Date'], '%Y-%m-%d').strftime('%B %d, %Y')})\"\n",
    "                return row[r]\n",
    "\n",
    "        return (null_value,)\n",
    "\n",
    "    def get_revue(row, null_value=\"\"):\n",
    "        \"\"\"(internal) for use with DataFrame lambda function to return the cleaned-up version of a revue's name (in an order of priority)\"\"\"\n",
    "        for r in [\"Normalized Revue Name\", \"Revue name\"]:\n",
    "            if row[r]:\n",
    "                return row[r]\n",
    "\n",
    "        return null_value\n",
    "\n",
    "    df[\"Performer\"] = df.apply(lambda row: get_performer(row), axis=1)\n",
    "    df[\"City\"] = df.apply(lambda row: get_city(row), axis=1)\n",
    "    df[\"Source\"] = df.apply(lambda row: get_source(row), axis=1)\n",
    "    df[\"Revue\"] = df.apply(lambda row: get_revue(row), axis=1)\n",
    "    df[\"Unique venue\"] = df.apply(lambda row: get_unique_venue(row), axis=1)\n",
    "    log(f\"**Cleaned up all names**.\", verbose=verbose)\n",
    "\n",
    "    for col in drop_cols:\n",
    "        try:\n",
    "            del df[col]\n",
    "        except KeyError:\n",
    "            pass\n",
    "\n",
    "    df = df.rename(columns={\"Unique venue\": \"Venue\"})\n",
    "\n",
    "    log(\n",
    "        f\"**Fixed columns**: Renamed some columns and removed all unneccesary columns.\",\n",
    "        verbose=verbose,\n",
    "    )\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def get_clean_network_data(\n",
    "    min_date=None,\n",
    "    max_date=None,\n",
    "    drop_cols=None,\n",
    "    verbose=True,\n",
    "    url=\"https://docs.google.com/spreadsheets/d/e/2PACX-1vT0E0Y7txIa2pfBuusA1cd8X5OVhQ_D0qZC8D40KhTU3xB7McsPR2kuB7GH6ncmNT3nfjEYGbscOPp0/pub?gid=254069133&single=true&output=csv\",\n",
    "):\n",
    "    \"\"\"A \"collector\" function that runs through `get_raw_data`, `filter_data` and `clean_data` in that order and then resets the index.\"\"\"\n",
    "\n",
    "    df = get_raw_data(verbose=verbose, url=url)\n",
    "    df = filter_data(df, min_date=min_date, max_date=max_date, verbose=verbose)\n",
    "\n",
    "    if not drop_cols:\n",
    "        drop_cols = [\n",
    "            \"EIMA\",\n",
    "            \"Imported from former archive\",\n",
    "            \"Search (newspapers.com)\",\n",
    "            \"Search (fulton)\",\n",
    "            \"Venue\",\n",
    "            \"Revue name\",\n",
    "            \"Normalized Revue Name\",\n",
    "            \"Legal name\",\n",
    "            \"Alleged age\",\n",
    "            \"Assumed birth year\",\n",
    "            \"Source clean\",\n",
    "            \"Category\",\n",
    "            \"2020-12-31 ID\",\n",
    "            \"Normalized City\",\n",
    "            \"Performer first-name\",\n",
    "            \"Performer last-name\",\n",
    "            \"Normalized performer\",\n",
    "            \"has_required_data\",\n",
    "            \"has_correct_date\",\n",
    "            \"Exclude from visualization\",\n",
    "            \"Blackface\",\n",
    "            \"Sepia\",\n",
    "            \"Fan dancer/Sally Rand\",\n",
    "            \"Exotic/erotic/oriental dancer/Gypsy\",\n",
    "            \"Has image\",\n",
    "            \"Address\",\n",
    "            \"Vaudeville Circuit/Circus\",\n",
    "            \"Edge Comment\",\n",
    "            \"Comment on node: performer\",\n",
    "            \"Comment on node: venue\",\n",
    "            \"Comment on node: city\",\n",
    "            \"Comment on edge: revue\",\n",
    "            \"Normalized Venue\",\n",
    "        ]\n",
    "\n",
    "    df = clean_data(df, drop_cols, verbose=verbose)\n",
    "\n",
    "    df = df.reset_index(drop=True)\n",
    "    log(f\"**Index has been reset**.\", verbose=verbose)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def test_same_df(df1, df2):\n",
    "    try:\n",
    "        for cols in [[x for x in df1.columns], [x for x in df2.columns]]:\n",
    "            for col in cols:\n",
    "                for ix, row in (df1 == df2).iterrows():\n",
    "                    if not all([row[col] for col in cols]):\n",
    "                        return False\n",
    "                if not [x for x in df1[col]] == [x for x in df2[col]]:\n",
    "                    return False\n",
    "    except ValueError:\n",
    "        return False\n",
    "\n",
    "    return True\n",
    "\n",
    "\n",
    "def get_performers_who_were_there(df, where=None, when=[]):\n",
    "    \"\"\"Returns a list of all the performers from any list of dates and venue\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    How this function works:\n",
    "    get_performers_who_were_there(df, 'Band Box (Syracuse, NY)', ['1935-03-29', '1935-04-05', '1935-04-12', '1935-04-19'])\n",
    "    \"\"\"\n",
    "    if not isinstance(when, list):\n",
    "        when = [when]\n",
    "\n",
    "    all_values = []\n",
    "    for when in when:\n",
    "        if isinstance(when, datetime.datetime):\n",
    "            when = when.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        selected_rows = df[(df[\"Date\"] == when) & (df[\"Venue\"] == where)]\n",
    "\n",
    "        all_values.extend(selected_rows[\"Performer\"])\n",
    "\n",
    "    return sorted(list(set(all_values)))\n",
    "\n",
    "\n",
    "def group_dates(\n",
    "    dates: list = [], delta=datetime.timedelta(days=14), dateformat=\"%Y-%m-%d\"\n",
    "):\n",
    "    \"\"\"https://gist.github.com/kallewesterling/9a8d12ce073776ed52865bfb362ad073\"\"\"\n",
    "\n",
    "    \"\"\"\n",
    "    Chains dates together by looking for the delta between any given dates in a list\n",
    "    \n",
    "    Example:\n",
    "    \n",
    "    (A.) Provided that the delta is `days=14`,\n",
    "         the left side will generate the right side:\n",
    "            [                           [\n",
    "                1935-01-13,               [1935-01-13, 1935-01-26,\n",
    "                1935-01-26,                1935-02-11, 1935-02-05],\n",
    "                1935-02-11,\n",
    "                1935-02-05,\n",
    "                1935-04-01,               [1935-04-01, 1935-04-06]\n",
    "                1935-04-06\n",
    "            ]                           ]\n",
    "            \n",
    "    (B.) Provided that the delta is `days=3`,\n",
    "         the left side will generate the right side:\n",
    "            [                           [\n",
    "                1935-01-13,               [1935-01-13],\n",
    "                1935-01-26,               [1935-01-26],\n",
    "                1935-02-11,               [1935-02-11],\n",
    "                1935-02-05,               [1935-02-05],\n",
    "                1935-04-01,               [1935-04-01],\n",
    "                1935-04-06                [1935-04-06]\n",
    "            ]                           ]\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        dates = sorted([datetime.datetime.strptime(x, dateformat) for x in dates])\n",
    "    except ValueError as e:\n",
    "        date = re.search(r\"\"\"['\"](.*)['\"] does not match format\"\"\", str(e))\n",
    "        if date:\n",
    "            date = date.groups()[0]\n",
    "        raise ValueError(\n",
    "            f\"A date found in list that did not adhere to format (`{date}`). Needs to follow format `{dateformat}`.\"\n",
    "        ) from None\n",
    "\n",
    "    if isinstance(delta, int):\n",
    "        delta = datetime.timedelta(days=delta)\n",
    "\n",
    "    periods = []\n",
    "\n",
    "    for ix, date in enumerate(dates):\n",
    "        min_date = date - delta\n",
    "        max_date = date + delta\n",
    "\n",
    "        prev_date, next_date = None, None\n",
    "        start_chain, end_chain, in_chain, solo_date = None, None, None, None\n",
    "        prev_date_in_range, next_date_in_range = None, None\n",
    "\n",
    "        try:\n",
    "            if ix - 1 >= 0:\n",
    "                prev_date = dates[ix - 1]\n",
    "        except IndexError:\n",
    "            prev_date = None\n",
    "\n",
    "        try:\n",
    "            next_date = dates[ix + 1]\n",
    "        except IndexError:\n",
    "            next_date = None\n",
    "\n",
    "        if next_date:\n",
    "            next_date_in_range = next_date >= min_date and next_date <= max_date\n",
    "\n",
    "        if prev_date:\n",
    "            prev_date_in_range = prev_date >= min_date and prev_date <= max_date\n",
    "\n",
    "        if all([next_date, prev_date, prev_date_in_range, next_date_in_range]):\n",
    "            in_chain = True\n",
    "        elif all([next_date, prev_date, next_date_in_range]) and not prev_date_in_range:\n",
    "            start_chain = True\n",
    "        elif all([next_date, prev_date, prev_date_in_range]) and not next_date_in_range:\n",
    "            end_chain = True\n",
    "        elif all([next_date, prev_date]) and not all(\n",
    "            [prev_date_in_range, next_date_in_range]\n",
    "        ):\n",
    "            solo_date = True\n",
    "        elif next_date and next_date_in_range:\n",
    "            start_chain = True\n",
    "        elif next_date:\n",
    "            solo_date = True\n",
    "        elif prev_date and prev_date_in_range:\n",
    "            end_chain = True\n",
    "        elif prev_date:\n",
    "            solo_date = True\n",
    "        elif not next_date and not prev_date:\n",
    "            solo_date = True\n",
    "        else:\n",
    "            raise RuntimeError(\"An unexpected error occurred.\")\n",
    "\n",
    "        date_str = date.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "        if start_chain:\n",
    "            periods.append([date_str])\n",
    "\n",
    "        elif end_chain:\n",
    "            periods[len(periods) - 1].append(date_str)\n",
    "\n",
    "        elif solo_date:\n",
    "            periods.append([date_str])\n",
    "\n",
    "        elif in_chain:\n",
    "            periods[len(periods) - 1].append(date_str)\n",
    "\n",
    "    return periods\n",
    "\n",
    "\n",
    "def get_group_data(df, days=settings[\"DAYSPANS\"], verbose=False):\n",
    "    data_dict = {}\n",
    "\n",
    "    venue_count = len(df.groupby(\"Venue\"))\n",
    "    i = 1\n",
    "    for venue, row in df.groupby(\"Venue\"):\n",
    "        i += 1\n",
    "        for num_days in days:\n",
    "            log(\n",
    "                f'Generating group data for spans of {\", \".join([str(x) for x in days])} days.',\n",
    "                verbose=verbose,\n",
    "            )\n",
    "            log(\n",
    "                f\"   [{i}/{venue_count}] processing venue {venue} (date span {num_days} days)...\",\n",
    "                verbose=verbose,\n",
    "            )\n",
    "            clear_output(wait=True)\n",
    "            all_dates = list(set(row.Date))\n",
    "            grouped_dates = group_dates(\n",
    "                all_dates, delta=datetime.timedelta(days=num_days)\n",
    "            )\n",
    "            for ix, date_group in enumerate(grouped_dates, start=1):\n",
    "                if not venue in data_dict:\n",
    "                    data_dict[venue] = {}\n",
    "                if not f\"grouped-by-{num_days}-days\" in data_dict[venue]:\n",
    "                    data_dict[venue][f\"grouped-by-{num_days}-days\"] = {}\n",
    "\n",
    "                revues = list(set([x for x in row.Revue if x]))\n",
    "                cities = list(set([x for x in row.City if x]))\n",
    "\n",
    "                data_dict[venue][f\"grouped-by-{num_days}-days\"][f\"date_group-{ix}\"] = {\n",
    "                    \"dates\": date_group,\n",
    "                    \"performers\": get_performers_who_were_there(df, venue, date_group),\n",
    "                    \"revues\": revues,\n",
    "                    \"cities\": cities,\n",
    "                }\n",
    "    log(f\"Generated group data for {venue_count} venues.\", verbose=verbose)\n",
    "    return data_dict\n",
    "\n",
    "\n",
    "def drop_unnamed(n):\n",
    "    return not \"unnamed\" in n.lower()\n",
    "\n",
    "\n",
    "def get_meta_data(df, category=None, verbose=False):\n",
    "    meta_data = {\"performers\": {}, \"venues\": {}, \"cities\": {}, \"revues\": {}}\n",
    "\n",
    "    MAP = {\n",
    "        \"performers\": {\n",
    "            \"cleaned_row_name\": \"Performer\",\n",
    "            \"MAPPING\": {\n",
    "                \"comments\": \"Comment on node: performer\",\n",
    "                \"legal_names\": \"Legal name\",\n",
    "                \"alleged_ages\": \"Alleged age\",\n",
    "                \"assumed_birth_years\": \"Assumed birth year\",\n",
    "                \"images\": \"Has image\",\n",
    "                \"exotic_dancer\": \"Exotic/erotic/oriental dancer/Gypsy\",\n",
    "                \"fan_dancer\": \"Fan dancer/Sally Rand\",\n",
    "                \"blackface\": \"Blackface\",\n",
    "                \"sepia\": \"Sepia\",\n",
    "            },\n",
    "        },\n",
    "        \"cities\": {\n",
    "            \"cleaned_row_name\": \"City\",\n",
    "            \"MAPPING\": {\"comments\": \"Comment on node: city\"},\n",
    "        },\n",
    "        \"venues\": {\n",
    "            \"cleaned_row_name\": \"Venue\",\n",
    "            \"MAPPING\": {\"comments\": \"Comment on node: venue\"},\n",
    "        },\n",
    "        \"revues\": {\n",
    "            \"cleaned_row_name\": \"Revue\",\n",
    "            \"MAPPING\": {\"comments\": \"Comment on edge: revue\"},\n",
    "        },\n",
    "    }\n",
    "\n",
    "    for meta_data_category, d in MAP.items():\n",
    "        if category and not meta_data_category == category:\n",
    "            continue\n",
    "\n",
    "        log(\n",
    "            f\"Fetching node meta information for {meta_data_category}...\",\n",
    "            verbose=verbose,\n",
    "        )\n",
    "        for ix, row in df.iterrows():\n",
    "            if not row[d[\"cleaned_row_name\"]] in meta_data[meta_data_category]:\n",
    "                meta_data[meta_data_category][row[d[\"cleaned_row_name\"]]] = {}\n",
    "\n",
    "            for key, column_name in d[\"MAPPING\"].items():\n",
    "                if not key in meta_data[meta_data_category][row[d[\"cleaned_row_name\"]]]:\n",
    "                    meta_data[meta_data_category][row[d[\"cleaned_row_name\"]]][key] = []\n",
    "\n",
    "                if row[column_name]:\n",
    "                    source = row[\"Source\"]\n",
    "                    content = row[column_name]\n",
    "                    if isinstance(content, str) and content.lower() == \"true\":\n",
    "                        content = True\n",
    "\n",
    "                    meta_data[meta_data_category][row[d[\"cleaned_row_name\"]]][\n",
    "                        key\n",
    "                    ].append({\"source\": source, \"content\": content})\n",
    "\n",
    "    return meta_data\n",
    "\n",
    "\n",
    "def get_meta(\n",
    "    df=None,\n",
    "    category=None,\n",
    "    verbose=False,\n",
    "    url=\"https://docs.google.com/spreadsheets/d/e/2PACX-1vT0E0Y7txIa2pfBuusA1cd8X5OVhQ_D0qZC8D40KhTU3xB7McsPR2kuB7GH6ncmNT3nfjEYGbscOPp0/pub?gid=254069133&single=true&output=csv\",\n",
    "):\n",
    "    if not isinstance(df, pd.DataFrame):\n",
    "        log(\"Building new clean data for node meta information...\", verbose=verbose)\n",
    "        df = get_raw_data(verbose=False, url=url)\n",
    "        df = filter_data(df, max_date=None, min_date=None, verbose=False)\n",
    "        df = clean_data(df, drop_cols=[\"Venue\"], verbose=False)\n",
    "\n",
    "    all_meta = get_meta_data(df, category=category)\n",
    "\n",
    "    if not category:\n",
    "        return all_meta\n",
    "\n",
    "    return all_meta[category]\n",
    "\n",
    "\n",
    "def get_connected_nodes_per_node(G):\n",
    "    return {node: sorted(nx.bfs_tree(G, node, reverse=False).nodes) for node in G.nodes}\n",
    "\n",
    "\n",
    "def get_unique_networks(connected_nodes_per_node):\n",
    "    if isinstance(connected_nodes_per_node, dict):\n",
    "        pass\n",
    "    elif isinstance(connected_nodes_per_node, nx.classes.graph.Graph):\n",
    "        connected_nodes_per_node = get_connected_nodes_per_node(\n",
    "            connected_nodes_per_node\n",
    "        )\n",
    "    else:\n",
    "        raise RuntimeError(\n",
    "            \"connected_nodes_per_node provided must be either a dictionary of nodes connected together or a networkx Graph object.\"\n",
    "        )\n",
    "\n",
    "    unique_networks = []\n",
    "    for network in list(connected_nodes_per_node.values()):\n",
    "        if not network in unique_networks:\n",
    "            unique_networks.append(network)\n",
    "    return unique_networks\n",
    "\n",
    "\n",
    "def merge_community_dicts(*args):\n",
    "    _ = {}\n",
    "    for dictionary in args:\n",
    "        for performer, data in dictionary.items():\n",
    "            if not performer in _:\n",
    "                _[performer] = {}\n",
    "            for key, value in data.items():\n",
    "                if not key in _[performer]:\n",
    "                    if isinstance(value, dict):\n",
    "                        _[performer][key] = {}\n",
    "                    else:\n",
    "                        raise NotImplemented(\"Nope\")\n",
    "                for key2, value2 in value.items():\n",
    "                    if not key2 in _[performer][key]:\n",
    "                        _[performer][key][key2] = value2\n",
    "                    else:\n",
    "                        raise NotImplemented(\"This should not happen\")\n",
    "\n",
    "    return _\n",
    "\n",
    "\n",
    "def get_degrees(G, node):\n",
    "    indegree = sum([1 for edge in G.edges if edge[0] == node])\n",
    "    outdegree = sum([1 for edge in G.edges if edge[1] == node])\n",
    "    degree = indegree + outdegree\n",
    "\n",
    "    return {\"indegree\": indegree, \"outdegree\": outdegree, \"degree\": degree}\n",
    "\n",
    "\n",
    "for url_data in urls:\n",
    "    PREFIX = url_data[\"prefix\"]\n",
    "    URL = url_data[\"url\"]\n",
    "\n",
    "    df = get_clean_network_data(\n",
    "        min_date=datetime.datetime(year=1930, month=1, day=1),\n",
    "        max_date=datetime.datetime(year=1940, month=12, day=31),\n",
    "        verbose=False,\n",
    "        url=URL,\n",
    "    )\n",
    "\n",
    "    group_data_dict = get_group_data(df)\n",
    "\n",
    "    metadata = {}\n",
    "\n",
    "    df_grouped_dates = pd.DataFrame()\n",
    "\n",
    "    venue_span_data = {}\n",
    "\n",
    "    for venue, row in tqdm(\n",
    "        df.groupby(\"Venue\"),\n",
    "        bar_format=\"Generating date data for venues: {n_fmt}/{total_fmt} {bar}\",\n",
    "        colour=\"green\",\n",
    "    ):\n",
    "        d = {}\n",
    "        for days in [3, 14, 31, 93, 186, 365]:\n",
    "            all_dates = list(set(row.Date))\n",
    "            grouped_dates = group_dates(all_dates, delta=datetime.timedelta(days=days))\n",
    "            max_span = 0\n",
    "            max_performers_in_date_group = 0\n",
    "            group_member_counters = Counter()\n",
    "            for date_group in grouped_dates:\n",
    "                venue_span_data[str(date_group)] = {}\n",
    "                performers_in_date_group = []\n",
    "                last_day_in_date_group = max(\n",
    "                    [datetime.datetime.strptime(x, \"%Y-%m-%d\") for x in date_group]\n",
    "                )\n",
    "                first_day_in_date_group = min(\n",
    "                    [datetime.datetime.strptime(x, \"%Y-%m-%d\") for x in date_group]\n",
    "                )\n",
    "                datespan = (last_day_in_date_group - first_day_in_date_group).days\n",
    "                if datespan > max_span:\n",
    "                    max_span = datespan\n",
    "                for performer in [\n",
    "                    get_performers_who_were_there(df, where=venue, when=x)\n",
    "                    for x in date_group\n",
    "                ]:\n",
    "                    performers_in_date_group.extend(performer)\n",
    "                performers_in_date_group = list(set(performers_in_date_group))\n",
    "                if len(performers_in_date_group) > max_performers_in_date_group:\n",
    "                    max_performers_in_date_group = len(performers_in_date_group)\n",
    "                group_member_counters[len(performers_in_date_group)] += 1\n",
    "\n",
    "            d[f\"num_groups (#, delta: {days} days)\"] = len(grouped_dates)\n",
    "            d[f\"max_span (days, delta: {days} days)\"] = max_span\n",
    "            d[\n",
    "                f\"max performers in a group (#, delta: {days} days)\"\n",
    "            ] = max_performers_in_date_group\n",
    "            d[\n",
    "                f\"group_member_counters for venue (#, delta: {days} days)\"\n",
    "            ] = group_member_counters\n",
    "        s = pd.Series(d, name=venue)\n",
    "        df_grouped_dates = df_grouped_dates.append(s)\n",
    "        dtype = {\n",
    "            key: int\n",
    "            for key in [\n",
    "                x for x in d.keys() if not \"group_member_counters for venue\" in x\n",
    "            ]\n",
    "        }\n",
    "        df_grouped_dates = df_grouped_dates.astype(dtype)\n",
    "\n",
    "    metadata[\"grouped_dates\"] = df_grouped_dates[list(d.keys())].T.to_json()\n",
    "\n",
    "    networks = {}\n",
    "\n",
    "    venue_count = len(group_data_dict)\n",
    "\n",
    "    for venue, data in tqdm(\n",
    "        group_data_dict.items(),\n",
    "        bar_format=\"Generating data for network edges and nodes: {n_fmt}/{total_fmt} {bar}\",\n",
    "        colour=\"green\",\n",
    "    ):\n",
    "        for grouped_by, data2 in data.items():\n",
    "            clear_output(wait=True)\n",
    "            if not grouped_by in networks:\n",
    "                networks[grouped_by] = nx.Graph()\n",
    "                networks[grouped_by].generated = datetime.datetime.now()\n",
    "\n",
    "            for date_group_id, data3 in data2.items():\n",
    "                if len(data3[\"performers\"]) > 1:\n",
    "                    performers = data3[\"performers\"]\n",
    "                    dates = data3[\"dates\"]\n",
    "                    revues = data3[\"revues\"]\n",
    "                    cities = data3[\"cities\"]\n",
    "                    for performer in performers:\n",
    "                        for target in [x for x in performers if not x == performer]:\n",
    "                            edge = (performer, target)\n",
    "                            if not edge in networks[grouped_by].edges:\n",
    "                                networks[grouped_by].add_edges_from(\n",
    "                                    [edge], coLocated={}\n",
    "                                )\n",
    "                            if (\n",
    "                                not venue\n",
    "                                in networks[grouped_by].edges[edge][\"coLocated\"]\n",
    "                            ):\n",
    "                                networks[grouped_by].edges[edge][\"coLocated\"][\n",
    "                                    venue\n",
    "                                ] = []\n",
    "                            if (\n",
    "                                not dates\n",
    "                                in networks[grouped_by].edges[edge][\"coLocated\"][venue]\n",
    "                            ):\n",
    "                                networks[grouped_by].edges[edge][\"coLocated\"][\n",
    "                                    venue\n",
    "                                ].append(dates)\n",
    "\n",
    "                            if not \"revues\" in networks[grouped_by].edges[edge]:\n",
    "                                networks[grouped_by].edges[edge][\"revues\"] = []\n",
    "                            if not revues in networks[grouped_by].edges[edge][\"revues\"]:\n",
    "                                networks[grouped_by].edges[edge][\"revues\"].extend(\n",
    "                                    revues\n",
    "                                )\n",
    "                                networks[grouped_by].edges[edge][\"revues\"] = list(\n",
    "                                    set(networks[grouped_by].edges[edge][\"revues\"])\n",
    "                                )\n",
    "\n",
    "                            if not \"cities\" in networks[grouped_by].edges[edge]:\n",
    "                                networks[grouped_by].edges[edge][\"cities\"] = []\n",
    "                            if not cities in networks[grouped_by].edges[edge][\"cities\"]:\n",
    "                                networks[grouped_by].edges[edge][\"cities\"].extend(\n",
    "                                    cities\n",
    "                                )\n",
    "                                networks[grouped_by].edges[edge][\"cities\"] = list(\n",
    "                                    set(networks[grouped_by].edges[edge][\"cities\"])\n",
    "                                )\n",
    "\n",
    "    _networks = {}\n",
    "\n",
    "    for key in tqdm(\n",
    "        networks.keys(),\n",
    "        bar_format=\"Generating networks: {n_fmt}/{total_fmt} {bar}\",\n",
    "        colour=\"green\",\n",
    "    ):\n",
    "        _networks[key] = copy.deepcopy(networks[key])\n",
    "        _networks[f\"{key}-no-unnamed-performers\"] = nx.subgraph_view(\n",
    "            _networks[key], filter_node=drop_unnamed\n",
    "        )\n",
    "        _networks[f\"{key}-no-unnamed-performers\"].generated = datetime.datetime.now()\n",
    "\n",
    "    networks = _networks\n",
    "\n",
    "    for key in tqdm(\n",
    "        networks.keys(),\n",
    "        bar_format=\"Adding edges: {n_fmt}/{total_fmt} {bar}\",\n",
    "        colour=\"green\",\n",
    "    ):\n",
    "        for edge in list(networks[key].edges):\n",
    "            networks[key].edges[edge][\"weights\"] = {}\n",
    "            for co_located, date_groups in (\n",
    "                networks[key].edges[edge][\"coLocated\"].items()\n",
    "            ):\n",
    "                networks[key].edges[edge][\"weights\"][\"dateGroups\"] = len(date_groups)\n",
    "            networks[key].edges[edge][\"weights\"][\"venues\"] = len(\n",
    "                networks[key].edges[edge][\"coLocated\"]\n",
    "            )\n",
    "\n",
    "    all_meta = get_meta(url=URL)\n",
    "    metadata[\"content\"] = all_meta\n",
    "\n",
    "    for key in tqdm(\n",
    "        networks.keys(),\n",
    "        bar_format=\"Adding metadata: {n_fmt}/{total_fmt} {bar}\",\n",
    "        colour=\"green\",\n",
    "    ):\n",
    "        nx.set_node_attributes(networks[key], all_meta[\"performers\"])\n",
    "\n",
    "    for key in tqdm(\n",
    "        networks.keys(),\n",
    "        bar_format=\"Generating metadata for connected nodes in each network: {n_fmt}/{total_fmt} {bar}\",\n",
    "        colour=\"green\",\n",
    "    ):\n",
    "        unique_networks = get_unique_networks(networks[key])\n",
    "\n",
    "        for network_id, unique_network in enumerate(unique_networks, start=1):\n",
    "            for performer in unique_network:\n",
    "                networks[key].nodes[performer][\"connected\"] = {\n",
    "                    \"network\": {\n",
    "                        \"nodes\": [x for x in unique_network if not x == performer],\n",
    "                        \"network_id\": network_id,\n",
    "                    }\n",
    "                }\n",
    "\n",
    "    for key in tqdm(\n",
    "        networks.keys(),\n",
    "        bar_format=\"Generating modularities for network: {n_fmt}/{total_fmt} {bar}\",\n",
    "        colour=\"green\",\n",
    "    ):\n",
    "        tqdm.write(f\"{' '*len(PREFIX)}   --> Louvain for `{key}`\")\n",
    "        louvain = community_louvain.best_partition(networks[key])\n",
    "        louvain = {\n",
    "            performer: {\"modularities\": {\"Louvain\": community_number}}\n",
    "            for performer, community_number in louvain.items()\n",
    "        }\n",
    "\n",
    "        tqdm.write(f\"{' '*len(PREFIX)}   --> CNM for `{key}`\")\n",
    "        c = nx.community.greedy_modularity_communities(networks[key])\n",
    "        clauset_newman_moore = {\n",
    "            performer: {\"modularities\": {\"Clauset-Newman-Moore\": community_number}}\n",
    "            for community_number, list_of_performers in enumerate(c, start=1)\n",
    "            for performer in list_of_performers\n",
    "        }\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO: This won't work\n",
    "        gn = nx.community.girvan_newman(networks[key])\n",
    "        first_girvan_newman_iteration = next(gn)\n",
    "        girvan_newman_groups = {group: names for group, names in enumerate([list(x) for x in first_girvan_newman_iteration], start=1)}\n",
    "        \"\"\"\n",
    "\n",
    "        community_dicts = merge_community_dicts(louvain, clauset_newman_moore)\n",
    "\n",
    "        nx.set_node_attributes(networks[key], community_dicts)\n",
    "\n",
    "    for key in tqdm(\n",
    "        networks.keys(),\n",
    "        bar_format=\"Setting modularities on network metadata: {n_fmt}/{total_fmt} {bar}\",\n",
    "        colour=\"green\",\n",
    "    ):\n",
    "        for performer in networks[key].nodes:\n",
    "            networks[key].nodes[performer][\"centralities\"] = {}\n",
    "\n",
    "        for performer, degree in nx.degree_centrality(networks[key]).items():\n",
    "            networks[key].nodes[performer][\"centralities\"][\n",
    "                \"degree_centrality_100x\"\n",
    "            ] = round(degree * 100, 6)\n",
    "\n",
    "        for performer, degree in nx.betweenness_centrality(\n",
    "            networks[key], k=len(networks[key].nodes)\n",
    "        ).items():\n",
    "            networks[key].nodes[performer][\"centralities\"][\n",
    "                \"betweenness_centrality_100x\"\n",
    "            ] = round(degree * 100, 6)\n",
    "\n",
    "        for performer, degree in nx.eigenvector_centrality(\n",
    "            networks[key], max_iter=1000, weight=\"weight\"\n",
    "        ).items():\n",
    "            networks[key].nodes[performer][\"centralities\"][\n",
    "                \"eigenvector_centrality_100x\"\n",
    "            ] = round(degree * 100, 6)\n",
    "\n",
    "        # try:\n",
    "        #    for performer, degree in nx.katz_centrality(networks[key]).items():\n",
    "        #        networks[key].nodes[performer]['centralities']['katz_centrality_100x'] = round(degree*100, 6)\n",
    "        # except nx.exception.PowerIterationFailedConvergence as e:\n",
    "        #    print(f'Katz Centrality failed: {e}')\n",
    "\n",
    "        for performer, degree in nx.closeness_centrality(networks[key]).items():\n",
    "            networks[key].nodes[performer][\"centralities\"][\n",
    "                \"closeness_centrality_100x\"\n",
    "            ] = round(degree * 100, 6)\n",
    "\n",
    "    for key in tqdm(\n",
    "        networks.keys(),\n",
    "        bar_format=\"Setting degrees on network metadata: {n_fmt}/{total_fmt} {bar}\",\n",
    "        colour=\"green\",\n",
    "    ):\n",
    "        degrees = {\n",
    "            node: {\"degrees\": get_degrees(networks[key], node)}\n",
    "            for node in networks[key].nodes\n",
    "        }\n",
    "        nx.set_node_attributes(networks[key], degrees)\n",
    "\n",
    "    for key, network in tqdm(\n",
    "        networks.items(),\n",
    "        bar_format=\"Correcting last-minute data for networks: {n_fmt}/{total_fmt} {bar}\",\n",
    "        colour=\"green\",\n",
    "    ):\n",
    "        for node in networks[key].nodes:\n",
    "            networks[key].nodes[node][\"node_id\"] = slugify(node)\n",
    "            networks[key].nodes[node][\"category\"] = \"performer\"\n",
    "            networks[key].nodes[node][\"display\"] = node\n",
    "\n",
    "        for edge in networks[key].edges:\n",
    "            networks[key].edges[edge][\"edge_id\"] = slugify(f\"{edge[0]}-{edge[1]}\")\n",
    "            networks[key].edges[edge][\"comments\"] = []\n",
    "            networks[key].edges[edge][\"general_comments\"] = []\n",
    "\n",
    "            networks[key].edges[edge][\"found\"] = []\n",
    "            for _, dates in networks[key].edges[edge][\"coLocated\"].items():\n",
    "                for datelist in dates:\n",
    "                    for date in datelist:\n",
    "                        if not date in networks[key].edges[edge][\"found\"]:\n",
    "                            networks[key].edges[edge][\"found\"].append(date)\n",
    "\n",
    "            networks[key].edges[edge][\"comments\"] = {\n",
    "                \"venues\": {},\n",
    "                \"cities\": {},\n",
    "                \"revues\": {},\n",
    "            }\n",
    "\n",
    "        networks[grouped_by].finished = datetime.datetime.now()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
